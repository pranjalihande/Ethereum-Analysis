# Ethereum-Analysis
Analysis of Ethereum Transactions and Smart Contracts

Part A. Time Analysis (25%)
Task 1: Create a bar plot showing the number of transactions occurring every month between the start and end of the dataset. Approach: • The blocks dataset contains the transaction count with the time of the transaction. • The timestamp and the transaction count are mapped from the dataset using the map function. • With the second map function, the timestamp is used as a key, and the transaction count has been used as a value pair. • This data is then reduced using the ‘reduceByKey’ method, which collects all the distinct key values (unique month and year) with the total number of transactions that occurred each month and year. This resulting data is then sorted according to the key, i.e., timestamp (Year-Month) • The resulting sorted data is saved in the ‘/final_result.txt file’. This file plots a graph using Matplotlib where sorted data by year-month and the count of transactions represents the height of each bar in the plot. • Note: The same data can be fetched through the transaction’s dataset. I have tried both ways and found the same result. Code Snippet:
Output: Bar Plot: Implementation of this graph can be found in part_A.ipynb file. Observations: The number of transactions increases suddenly from a very low value and reaches its highest in 2018-01. After this, the transaction count has been decreasing again. Task 2: Create a bar plot showing the average value of transactions in each month between the start and end of the dataset. Approach: • In order to get the average value of transactions in each month, data from the transaction’s dataset is used. • Firstly, by using the map function, we are extracting the value for each transaction and the timestamp for each transaction. (Timestamp, Transaction value) • For the second mapper, we are using a timestamp with the unique month in the dataset as a key and transaction value and its count as a value. (Timestamp, (Transaction value,1)).
• The average is then calculated by reducing the number of key instances with their associated values by summing them up. Here we had 2 values which are, transaction value and count. The average is calculated by dividing the sum of values by the sum of the transaction count. • The resulting data is then sorted by key using the ‘sortByKey’ method. • This sorted data is copied to file ‘/final_avg_result.txt’ to plot the graph. • The graph is plotted using Matplotlib where the sorted data by Year-Month and plotted against the average transaction count. Code Snippet: Output: Bar Plot: Implementation of this graph can be found in part_A.ipynb file.
Observations: From the above bar plot, we can infer that; Number of average transactions was higher in the Year 2015-08. After which they have dropped drastically to a very low value and continue too same. After the 2018 year, we can see a very negligible average transaction count. Part B. Top Ten Most Popular Services (25%)
Evaluate the top 10 smart contracts by total Ether received. You will need to join address field in the contracts dataset to the to_address in the transactions dataset to determine how much ether a contract has received. Approach: • To evaluate the top 10 smart contracts by total ether received we need the value of ether which is present in the Transaction dataset. Due to this, we need to join the contracts dataset with the transaction dataset. • First, the ‘address’ column data is extracted from the contract dataset such as (Address, 1) using the map method. • From the Transaction dataset, ‘To address’ and ‘Value’ (Value transferred in Wei) is extracted using the map method. • The join operation is then performed on these two datasets over the address column as the key. As a result of join operation, we received data with the address and their ether value. • This data is then reduced to sum all the values of ether received for each address.
• In order to get the top 10 smart contracts, this data is ordered according to the value of ether received by using ‘takeOrdered’ method. This method is provided with ‘10’ as the first input as we need the top 10 contracts and then order descending to get those 10 contracts that have the highest ether value received. • These top 10 contracts are saved in a text file named ‘part_B_top_10.txt’. The details of the file are as per below image: Output:
Figure 1. Top Ten Most Popular Services Code Snippet:
Graph Plot:
Part C. Top Ten Most Active Miners (10%)
Evaluate the top 10 miners by the size of the blocks mined. This is simpler as it does not require a join. You will first have to aggregate blocks to see how much each miner has been involved in. You will want to aggregate size for addresses in the miner field. This will be similar to the wordcount that we saw in Lab 1 and Lab 2. You can add each value from the reducer to a list and then sort the list to obtain the most active miners. Approach:
• To get the most active miners by the size of blocks mined, we need to extract the Miner (The address of the beneficiary to whom the mining rewards were given) and the size (The size of this block in bytes) of the block from the blocks.csv file. • We got the miner and their activity by mapping each block size to the miner as a key in the given blocks dataset. Such as, (Miner, Size). • This data is reduced to a number of key instances by summing the size value for each miner, to get the block size mined by each miner. • The resulting data is sorted according to the size value and provides us top 10 active miners. • These top 10 most active miners are saved in a text file named ‘part_C_top_10.txt’. The details of the file are as per below output image.
Code Snippet: Output:
Figure 2. Top 10 most active miners Graph Plot:
Part D. Data exploration (40%)
Scam Analysis:
1. Popular Scams:
Utilising the provided scam dataset, what is the most lucrative form of scam? How does this change throughout time, and does this correlate with certain known scams going offline/inactive? To obtain the marks for this category you should provide the id of the most lucrative scam and a graph showing how the ether received has changed over time for the dataset. (20%/40%) Approach:
To achieve this, we needed data from the scams.json file. I have converted the given scams.json file to a scams.csv file by expanding the list of addresses to a single address. This converted scams.csv file is then copied to my local bucket. This file has the same data as the provided scams.csv. • First, Address, Category, status, and ID from the scams.csv are extracted using the map method with ‘Address’ as the key and others as values. E.g. ('0x00e01a648ff41346cdeb873182383333d2184dd1', ('Phishing', 'Offline', '130')) • To Address, Value, Block_Timestamp are extracted from the transaction.csv. E.g. ('0x6baf48e1c966d16559ce2dddb616ffa72004851e', (5000000000000000.0, '2015-08')) • These two datasets are then joined with Address as a key and all others as a value. Such as, (Address, (Value, Timestamp), (Category, status, ID)) E.g. ('0x01f7583ce239ad19682ccbadb21d69a03cf7be333f4be9c02233874e3f79bf9d', ((5000000000000000.0, '2015-08'), ('Phishing', 'Offline', '130')))
Code Snippet: A. What is the most lucrative form of scam? • To get the ‘The most lucrative form of scam’, the category and the values are mapped with the category being key. This is done with the help of the map method. • The data is then reduced by key by summing up these values for each unique category. This data is sorted with the value to get the most lucrative form of scam. • This data is saved in a file ‘most_lucrative_scam_form.txt’. Code Snippet: Output
Figure 3. The most lucrative form of scam The most lucrative form of scam?
Phishing: 4.321856144727656e+22 B. How does this change throughout time: • To understand how the most lucrative scam changes through time, we need to collect the category and Value and understand how it changes with time. • The mapper is used to extract this data with category and time as a key and transaction value as value, such as ((category, time), value). • This transaction value is then summed up for each unique category for a given timestamp. This is then reduced by using reduceByKey. • The resulting data is saved in the file, get_cat.txt. This data is then plotted with a timestamp on X-axis and a value on Y-axis. Output Sample: Code Snippet: Graph plot:
Observation: Phishing: Ether Value has increased suddenly from 0 in 2017-06 to the highest value in 2017-08. This dropped drastically after 2017-08 and gradually decrease to 0. Scamming: Ether value has been gradually changing from 0 to 0.25 from 2017-06 till 2018-08. This drastically increased in 2018-09 till the highest value and decreases to 0. Fake ICO: The ether value has been almost consistent with values less than 0.25 from 2017-06 till 2018-06. There is no ether that has been received after that. C. Does this correlate with certain known scams going offline/inactive? • In order to understand how the different categories, correlate with scams going offline/inactive, we need to fetch the status with category and time with respect to ether value. • With map method, we fetch this data using Category, status, and Time as a key and ether value as a value. ((Category, Status, Time), Value) • This transaction value is then summed up for each unique category and status for a given timestamp. This is then reduced by using reduceByKey. • The resulting data is saved in the file, get_status.txt. This data is then plotted with a timestamp on X-axis and a value on Y-axis. Output Sample:
Code Snippet: Graph plot:
• We can also get the same values when we map the data for category and Status with respect to value. Such as ((Category, Status), Value). This will provide us sum of all the values for the unique status and category by using the reducer. Code Snippet:
Graph Plot: Observation: From all the above graphs plots, we can observe that for suspended and inactive scam status we don’t see all categories present. For Active scam status, we can observe that “Scamming” has more value than “Phishing”, For Offline scam status, we can infer that it has the highest value for the “Phishing” category and after that, the value reduces further for Scamming and Fake ICO. D. The id of the most lucrative scam: • To get the ID of the most lucrative scam, we need to extract the ID and the ether value for all transactions in the dataset. • The map method is used to fetch the transaction ID and value, such as (ID, value), with ID as a key and ether value as a value. • By using the reducer method (reduceByKey), we add all the values for each unique ID. • This result data is then sorted according to the value. • In order to get the ID of the most lucrative scam, will only fetch the first value in the dataset along with its ID. • This data is saved in the file ‘most_lucarative_id_val.txt’. Output: ID of the most lucrative scam:
Code Snippet: E. How the ether received has changed over time? • To understand how ether received has changed over time, we need to extract the data for the Timestamp and ether value. • By using the map method, we fetch the data for Timestamp and Ether value. • With the help of the reducer, we sum up all the values for a particular Month-Year. • Then we need to sort this data according to the timestamp in ascending order. • This data is saved in ‘ether_recived_over_time.txt’ file. Output: Code Snippet:
Graph Plot: Observation: We can observe from the above graph that initially the ether value increased drastically till August 2017. After this, the values decreased suddenly and remain low till August 2018. Again, we can see the spike in value in September 2018 and reach its lowest again. Part D. Data exploration (40%)
Miscellaneous Analysis 1.Gas Guzzlers: For any transaction on Ethereum a user must supply gas. How has gas price changed over time? Have contracts become more complicated, requiring more gas, or less so? How does this correlate with your results seen within Part B. To obtain these marks you should provide a graph showing how gas price has changed over time, a graph showing how gas used for contract transactions has changed over time and identify if the most popular contracts use more or less than the average gas_used. (20%/40%) Approach:
• In order to get the details of gas guzzlers, we need to get the data from contracts.csv and Transaction.csv. • From contracts.csv, we will get the data for: (Address, count) using the Map method. • From Transaction.csv, we will get the data for: (Address, gas_price, gas, time, value) using the Map method. • We will join these two columns in order to get the common address between these two datasets. Code Snippet: A. How has gas price changed over time? • To understand how gas price changed over time, we need to get the data for gas price over each year-month. • To do that, we first used the map function to map the values of gas price and their count with timestamps. Here timestamp will be the key and the gas price, and its count will be the value. (Time, (Gas price, 1)) • With the help of reduce method ‘reduceByKey’, we will sum all the gas prices and their counts for each unique Year-Month timestamp. • To understand how gas price changed over time, will take an average of gas price by dividing it by its count. • This data is saved in file: ‘/average_gas_used_per_month.txt’. Code Snippet:
Output: Graph Plot: (How gas price has changed over time?) Observation: From the above graph, we can observe that the gas price has reduced over time increases. We can notice that gas price was their highest in August 2018 and continuously reduced with time and reach their lowest in January 2019. B. Have contracts become more complicated, requiring more gas, or less so? • In order to understand whether contracts become more complicated, we need to verify whether contracts are requiring more gas over time. • To obtain these results, we used the map method to map Timestamp and gas used from the joined result. In this case, timestamp (Year-Month) will be the kay and Gas will be the value. • With the help of Reduce method, we then sum all the gas used for each month. • The resulting data is saved in the file: ‘/average_gas_used_per_month.txt’ Code Snippet:
Output: Graph plot: (How gas used for contract transactions has changed over time) Observation: From the above graph, we can conclude that, as time increases gas use is getting increased. That means we can infer that the contracts have become more complicated. C. How does this correlate with your results seen within Part B. • To understand the correlation with Part B, we first need to get the data we generated in Part B. • In part B, we got the data for the top 10 smart contracts by total Ether received. In this case, we are getting the values of ether received for all the contracts. • To get this, we are extracting the timestamp and the value using the map method. • Will then use reduce method to get summation for ether received for each Year-Month. • This data will be sorted ascendingly on ether received value. • Resulting data is saved in file: ‘/time_with_values_used.txt’ Code Snippet:
Output: Graph Plot: Observation: From the above plot, we can observe that, as time increases the ether received values also increase. That means we can infer the contracts getting complicated are negatively correlated with the ether value received. D. Identify if the most popular contracts use more or less than the average gas_used. • In order to identify if the most popular contracts use more or less than the average gas_used we need to get the most popular contracts and Average gas used. • First, we will get the most popular contracts by fetching Address as a key and value, gas, and count as value. Such as, ((address, (value, gas, 1))) by using the Map method on the joined result we obtained earlier.
• With the help of reduce method, we will get the summation of all the values, gas and count for each address. • In order to get the average gas used for each address we will further use the map method by dividing the gas by the count. (Address, (value, gas/count)) • We will sort these data with values. • To get the top 10 contracts with the highest value first will use ‘takeOrdered’ on the basis of value. • The result is saved in file: ‘/top10address_avg_gas_value.txt’. Code Snippet: Output: • To get the overall average gas used, we need to map the gas value only with its count. • With the reducer method we will take a summation of all gas values and a summation of the count. • To get the average gas, we need to divide the gas value by the count. • This data is saved in file: ‘/total_average_gas_used.txt’ Code Snippet: Output: [["key", 231728.5239958902]]
Graph Plot: Observation: From the above plot, we can observe that the most popular contracts use less gas than the average gas_used. The blue dotted line shows the average gas used. All the top contracts use gas less than the average gas used. 2.Data Overhead: The blocks table contains a lot of information that may not strictly be necessary for a functioning cryptocurrency e.g. logs_bloom, sha3_uncles, transactions_root, state_root, receipts_root. Analyse how much space would be saved if these columns were removed. Note that all of these values are hex_strings so you can assume that each character after the first two requires four bits (this is not the case in reality but it would be possible to implement it like this) as this will affect your calculations. (20%/40%) Approach:
• To calculate the space saved by removing columns logs_bloom, sha3_uncles, transactions_root, state_root, receipts_root, we first need to get the space used by each of the columns. • We have written a separate method to calculate the size of column data, (get_size). • In this method, first we will calculate the length of column data using the ‘len’ function. • As given in the question, we need to ignore the first 2 hex string. In order to do that, we need to subtract 2 from the length. • As each character requires four bits, we will multiply this value by four. • If the column data is empty i.e., length is 0, in that case, we don’t need to calculate the size. Hence, we have added the check to calculate size only when the column has data else, we can return 0. Code Snippet: • This size will be then used in the map method to calculate the size of each column of data. • The size of each column will be then summed up using the reduce method. The resulting data will have the total size used by each column. • In order to get the total saved space, we need to add all these five values again. • This can be done using the map method on this result. • The total saved space is saved in file: ‘/saved_space.txt’. Code Snippet: Output:
The total bits size: 21504003276 Total size saved in GB: 2.6880004095 ~ 2.69GB
